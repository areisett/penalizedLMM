% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv-plmm.R
\name{cv.plmm}
\alias{cv.plmm}
\title{Cross-validation for plmm}
\usage{
cv.plmm(
  X,
  y = NULL,
  std_needed = NULL,
  col_names = NULL,
  k = NULL,
  K = NULL,
  diag_K = NULL,
  eta_star = NULL,
  penalty = "MCP",
  penalty.factor = NULL,
  type = "blup",
  gamma,
  alpha = 1,
  lambda.min,
  nlambda = 100,
  lambda,
  eps = 1e-04,
  max.iter = 10000,
  convex = TRUE,
  dfmax = NULL,
  warn = TRUE,
  init = NULL,
  cluster,
  nfolds = 10,
  seed,
  fold = NULL,
  returnY = FALSE,
  returnBiasDetails = FALSE,
  trace = FALSE,
  ...
)
}
\arguments{
\item{X}{Design matrix for model fitting. May include clinical covariates and other non-SNP data.}

\item{y}{Continuous outcome vector. Defaults to NULL, assuming that the outcome is the 6th column in the .fam PLINK file data. Can also be a user-supplied numeric vector.}

\item{std_needed}{Logical: does the supplied X need to be standardized? Defaults to FALSE, since \code{process_plink()} standardizes the design matrix by default.
By default, X will be standardized internally. For data processed from PLINK files, standardization happens in \code{process_plink()}. For data supplied as a matrix, standardization happens here in \code{plmm()}. If you know your data are already standardized, set \code{std_needed = FALSE} -- this would be an atypical case. \strong{Note}: failing to standardize data will lead to incorrect analyses.}

\item{col_names}{Optional vector of column names for design matrix. Defaults to NULL.}

\item{k}{An integer specifying the number of singular values to be used in the approximation of the rotated design matrix. This argument is passed to \code{RSpectra::svds()}. Defaults to \code{min(n, p) - 1}, where n and p are the dimensions of the \emph{standardized} design matrix.}

\item{K}{Similarity matrix used to rotate the data. This should either be (1) a known matrix that reflects the covariance of y, (2) an estimate (Default is \eqn{\frac{1}{p}(XX^T)}), or (3) a list with components 'd' and 'u', as returned by choose_k().}

\item{diag_K}{Logical: should K be a diagonal matrix? This would reflect observations that are unrelated, or that can be treated as unrelated. Defaults to FALSE.
Note: plmm() does not check to see if a matrix is diagonal. If you want to use a diagonal K matrix, you must set diag_K = TRUE.}

\item{eta_star}{Optional argument to input a specific eta term rather than estimate it from the data. If K is a known covariance matrix that is full rank, this should be 1.}

\item{penalty}{The penalty to be applied to the model. Either "MCP" (the default), "SCAD", or "lasso".}

\item{penalty.factor}{A multiplicative factor for the penalty applied to each coefficient. If supplied, penalty.factor must be a numeric vector of length equal to the number of columns of X. The purpose of penalty.factor is to apply differential penalization if some coefficients are thought to be more likely than others to be in the model. In particular, penalty.factor can be 0, in which case the coefficient is always in the model without shrinkage.}

\item{type}{A character argument indicating what should be returned from predict.plmm(). If type == 'lp', predictions are
based on the linear predictor, X beta. If type == 'blup', predictions are based on the sum of the linear predictor
and the estimated random effect (BLUP). Defaults to 'blup', as this has shown to be a superior prediction method
in many applications.}

\item{gamma}{The tuning parameter of the MCP/SCAD penalty (see details). Default is 3 for MCP and 3.7 for SCAD.}

\item{alpha}{Tuning parameter for the Mnet estimator which controls the relative contributions from the MCP/SCAD penalty and the ridge, or L2 penalty. alpha=1 is equivalent to MCP/SCAD penalty, while alpha=0 would be equivalent to ridge regression. However, alpha=0 is not supported; alpha may be arbitrarily small, but not exactly 0.}

\item{lambda.min}{The smallest value for lambda, as a fraction of lambda.max. Default is .001 if the number of observations is larger than the number of covariates and .05 otherwise.}

\item{nlambda}{Length of the sequence of lambda. Default is 100.}

\item{lambda}{A user-specified sequence of lambda values. By default, a sequence of values of length nlambda is computed, equally spaced on the log scale.}

\item{eps}{Convergence threshold. The algorithm iterates until the RMSD for the change in linear predictors for each coefficient is less than eps. Default is \code{1e-4}.}

\item{max.iter}{Maximum number of iterations (total across entire path). Default is 10000.}

\item{convex}{Calculate index for which objective function ceases to be locally convex? Default is TRUE.}

\item{dfmax}{Upper bound for the number of nonzero coefficients. Default is no upper bound. However, for large data sets, computational burden may be heavy for models with a large number of nonzero coefficients.}

\item{warn}{Return warning messages for failures to converge and model saturation? Default is TRUE.}

\item{init}{Initial values for coefficients. Default is 0 for all columns of X.}

\item{cluster}{cv.plmm() can be run in parallel across a cluster using the parallel package. The cluster must be set up in
advance using parallel::makeCluster(). The cluster must then be passed to cv.plmm().}

\item{nfolds}{The number of cross-validation folds. Default is 10.}

\item{seed}{You may set the seed of the random number generator in order to obtain reproducible results.}

\item{fold}{Which fold each observation belongs to. By default, the observations are randomly assigned.}

\item{returnY}{Should cv.plmm() return the linear predictors from the cross-validation folds? Default is FALSE; if TRUE,
this will return a matrix in which the element for row i, column j is the fitted value for observation i from
the fold in which observation i was excluded from the fit, at the jth value of lambda.}

\item{returnBiasDetails}{Logical: should the cross-validation bias (numeric value) and loss (n x p matrix) be returned? Defaults to FALSE.}

\item{trace}{If set to TRUE, inform the user of progress by announcing the beginning of each CV fold. Default is FALSE.}

\item{...}{Additional arguments to \code{plmm_fit}}
}
\value{
a list with 11 items:
\itemize{
\item type: the type of prediction used ('lp' or 'blup')
\item cve: numeric vector with the cross validation error (CVE) at each value of \code{lambda}
\item cvse: numeric vector with the estimated standard error associated with each value of for \code{cve}
\item fold: numeric \code{n} length vector of integers indicating the fold to which each observation was assigned
\item lambda: numeric vector of \code{lambda} values
\item fit: the overall fit of the object, including all predictors; this is a
list as returned by \code{plmm()}
\item min: The index corresponding to the value of \code{lambda} that minimizes \code{cve}
\item lambda.min: The \code{lambda} value at which \code{cve} is minmized
\item min1se: The index corresponding to the value of \code{lambda} within
standard error of that which minimizes \code{cve}
\item lambda1se: largest value of lambda such that error is within 1 standard error of the minimum.
\item null.dev: numeric value representing the deviance for the
intercept-only model. If you have supplied your own \code{lambda} sequence,
this quantity may not be meaningful.
}
}
\description{
Performs k-fold cross validation for lasso-, MCP-, or SCAD-penalized
linear mixed models over a grid of values for the regularization parameter \code{lambda}.
}
\examples{
cv_fit <- cv.plmm(X = admix$X, y = admix$y, seed = 321)
\dontrun{
cv_s <- summary.cv.plmm(cv_fit, lambda = "1se")
print(cv_s)
plot(cv_fit)

# filebacked example (file path is specific to current machine)
# since this dataset is < 100Mb, have to specify returnX = FALSE for 
# `get_data()` to return an FBM
my_fb_data <- paste0(plink_example(parent = T), "/penncath_lite")

cv_fb_fit <- cv.plmm(X = my_fb_data, type = 'blup', returnX = FALSE,
 trace = TRUE, nfolds = 3)

plot(cv_fb_fit)
summary(cv_fb_fit)
}



}
