---
title: "Analyze large data with filebacking"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analyze large data with filebacking}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(plmmr)
# MUST have biglasso version >= 1.5.2.1 - uncomment & run line below if needed
# remotes::install_github(repo = "YaohuiZeng/biglasso") 

library(biglasso)
library(bigsnpr)
```

In many applications of high dimensional data analysis, the dataset is too large to read into R -- the session will crash for lack of memory. To analyze such large datasets, `plmmr` is equipped to analyze data using *filebacking* - a strategy that lets R 'point' to a file on disk, rather than reading the file into the R session. 

Many other packages use this technique - [bigstatsr](https://privefl.github.io/bigstatsr/) and [biglasso](https://github.com/pbreheny/biglasso) are two examples of packages that use filebacking. The novelties of `plmmr` are: 

  (1) `plmmr` combines the functionality of several packages in order to do quality control, model fitting/analysis, and data visualization all in one package. `plmmr` will take you from PLINK files all the way to a list of SNPs for downstream analysis. 
  
  (2) `plmmr` uses a transformation that addresses correlation among samples and uses this correlation to improve predictions (via the best linear unbiased predictor, or BLUP). This means that in `plmm()`, there's no need to filter data down to a 'maximum subset of unrelated individuals.' 
  
  The tutorial that follows will walk you through each step of the process. We will use the `penncath_lite` data that ships with our `plmmr` package -- if you've installed this package, you have this dataset as a set of PLINK files. This dataset is small enough that we could read it into memory, but I will analyze it using filebacked methods here for the sake of example. More information about the `penncath_lite` data is given in the `PLINK files` article. 
  
  
# Analyzing filebacked data from start to finish

## Step 1: preprocess the data

```{r}
process_plink(data_dir = plink_example(parent = T),
              prefix = "penncath_lite",
              gz = TRUE,
              outfile = "process_penncath",
              overwrite = TRUE,
              impute_method = "mode")
```

You'll see a lot of messages printed to the console here ...

We can examine the RDS object created by `process_plink()` 

```{r}
pen <- snp_attach(paste0(plink_example(parent = T), "/penncath_lite.rds"))
str(pen) # note: genotypes and std_X are *not* in memory
```

## Step 2: fit a model 

Note: for data coming from `process_plink()`, the `y` argument in `plmm()` defaults to using the 6th column from the '.fam' file as the phenotype/outcome (this is the same as what PLINK does).

```{r}
my_fb_data <- paste0(plink_example(parent = T), "/penncath_lite")
fb_fit <- plmm(X = my_fb_data,
      returnX = FALSE,
      trace = TRUE)
```

In most applications, we will want to fit a model using cross validation 

```{r}
cv_fb_fit <- cv.plmm(X = my_fb_data,
                     type = 'blup',
                     returnX = FALSE,
                     trace = TRUE)
```


## Step 3: Plot/summarize results 

To summarize a single model (no CV), we can look at a plot of the coefficient paths: 

```{r}
plot(fb_fit)
```

We can also examine the number of selected variables at a particular index of $\lambda$: 
```{r}
summary(fb_fit, idx = 50)
```

To summarize and plot a CV fit, we can look at a plot of cross validation error: 

```{r}
plot(cv_fb_fit)
```

To summarize and visualize the model at the chosen $\lambda$ parameter value, we can do things like this:

```{r}
summary(cv_fb_fit)

plot(cv_fb_fit$fit, lambda = cv_fb_fit$lambda.min)
```

## Comparing candidate models 

The default penalty in `plmmr` is the minimax-concave penalty (MCP)^[Zhang, Cun-Hui. "Nearly unbiased variable selection under minimax concave penalty." (2010): 894-942.]. Suppose we want to compare our MCP-penalized model results with a lasso-penalized model. We can fit a lasso model as shown:

```{r}
fb_lasso <- plmm(X = my_fb_data,
                 penalty = "lasso",
                 returnX = FALSE,
                 trace = TRUE)

plot(fb_lasso)
summary(fb_lasso, idx = 50)


# with CV 
cv_fb_lasso <- cv.plmm(X = my_fb_data,
                       penalty = "lasso",
                     type = 'blup',
                     returnX = FALSE,
                     trace = TRUE)

plot(cv_fb_lasso)

summary(cv_fb_lasso)

plot(cv_fb_lasso$fit)
```

## To add an unpenalized covariate

In many biological applications, we want to include some features/covariates/predictors that are always in the model (i.e., they are not penalized). Let's suppose that in the `penncath` data analysis, we want to include 'sex' and 'age' as unpenalized covariates. We can do this by supplying an additional argument to `process_plink()`: 

```{r}
pen_clinic <- read.csv(paste0(plink_example(parent = T), "/penncath_clinical.csv"))
extdata <- pen_clinic[,3:4]
rownames(extdata) <- pen_clinic$FamID # This is important! 

process_plink(data_dir = plink_example(parent = T),
              prefix = "penncath_lite",
              id_var = "FID", # this is KEY!
              outfile = "process_penncath",
              overwrite = TRUE,
              impute_method = "mode",
              add_predictor_ext = extdata)

# check this out: 
pen2 <- snp_attach(paste0(plink_example(parent = T), "/penncath_lite.rds"))
pen2$genotypes |> dim() # original dimensions of the data, prior to any subsetting
pen2$std_X |> dim() # dimensions of the standardized and subset X 
pen2$std_X_colnames |> head() # std_X includes non-genomic covariates 
```

```{r}
dat_plus_newvars <- paste0(plink_example(parent = T), "/penncath_lite")
fb_fit <- plmm(X = dat_plus_newvars,
      returnX = FALSE,
      trace = TRUE)
```

