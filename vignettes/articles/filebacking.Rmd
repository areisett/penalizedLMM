---
title: "Analyze high-dimensional PLINK data with filebacking"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>") 
options(rmarkdown.html_vignette.check_title = FALSE)
# TODO: get the filepath issues sorted out so that I can set eval = TRUE
```

```{r setup}
library(plmmr)
# MUST have biglasso version >= 1.5.2.1 - uncomment & run line below if needed
# remotes::install_github(repo = "YaohuiZeng/biglasso") 

library(biglasso)
library(bigsnpr)
```

**Preliminary note**: rendering this vignette is proving to be tricky, given that these examples read in external data files. Right now, this renders on my local machine -- I am still working out the kinks of getting this to render for our website. For now, I've left `eval=FALSE` just so you all can at least see the code. Let me know if you have any questions. --Tabitha

In many applications of high dimensional data analysis, the dataset is too large to read into R -- the session will crash for lack of memory. This is almost always an issue with analyzing genetic data from [PLINK](https://www.cog-genomics.org/plink/) files. To analyze such large datasets, `plmmr` is equipped to analyze data using *filebacking* - a strategy that lets R 'point' to a file on disk, rather than reading the file into the R session. Many other packages use this technique - [bigstatsr](https://privefl.github.io/bigstatsr/) and [biglasso](https://github.com/pbreheny/biglasso) are two examples of packages that use filebacking. The novelties of `plmmr` are:

(1) **Integration**: `plmmr` combines the functionality of several packages in order to do quality control, model fitting/analysis, and data visualization all in one package. `plmmr` will take you from PLINK files all the way to a list of SNPs for downstream analysis.

(2) **Accessibility**: `plmmr` can be run from an `R` session on a typical desktop/laptop machine. The user does not need access to a supercomputer or experience with the command line in order to fit models `plmmr`. If you don't know what the 'command line' means, don't worry - `plmmr` was designed with you in mind :)

(3) **Handling correlation**: `plmmr` uses a transformation that addresses correlation among samples and uses this correlation to improve predictions (via the best linear unbiased predictor, or BLUP). This means that in `plmm()`, there's no need to filter data down to a 'maximum subset of unrelated individuals.'

The tutorial that follows will walk you through each step of the process. We will use the `penncath_lite` data that ships with our `plmmr` package -- if you've installed this package, you have this dataset as a set of PLINK files. These PLINK files represent about 1400 participants and 4,000+ SNPs. This dataset is small enough that we could read it into memory (4,000 is a small number of SNPs to study), but I will analyze it using filebacked methods here for the sake of example. The steps shown here would be the same for GWAS-sized datasets (100,000 - 800,000 SNPs). More information about the `penncath_lite` data is given in the `PLINK files` article. **Note**: if you haven't done so already, go ahead and [unzip the PLINK files that came with `plmmr`]{style="color:purple"}

If you are on MacOS or Linux, you can run this command to unzip: 
```{r, eval=FALSE}
temp_dir <- tempdir() # using a temp dir -- change to fit your preference
unzip_example_data(outdir = temp_dir)
```

# Analyzing filebacked data from start to finish

## Step 1: preprocess the data

The most important step is where you begin -- for GWAS data, we have to tell `plmmr` how to combine information across all three PLINK files (the `.bed`, `.bim`, and `.fam` files).

Here, we will create the files we want in a temporary directory just for the sake of example. Users can specify the folder of their choice for `rds_dir`, as shown below:

```{r}
# temp_dir <- tempdir() # using a temporary directory (if you didn't already create one above)
plink_data <- process_plink(data_dir = find_example_data(parent = TRUE), 
                            # find_example_data() will read in data that 'shipped' with the package, 
                            # once it has been unzipped (assuming you unzipped in the same folder)
                            data_prefix = "penncath_lite",
                            rds_dir = temp_dir,
                            rds_prefix = "imputed_penncath_lite",
                            overwrite = TRUE,
                            impute_method = "mode")
```

You'll see a lot of messages printed to the console here ... the result of all this is the creation of 3 files: `std_penncath_lite.rds` and `std_penncath_lite.bk` contain the data, and `process_plink.log` is a text file documenting the steps that were just done. These will show up in the folder where the PLINK data is; the file path to the .rds object is returned here to `plink_data`

We can examine what's in `std_penncath_lite.rds` using the `snp_attach()` function from the `bigsnpr` package:

```{r}
pen <- readRDS(plink_data)
str(pen) # note: genotype data is *not* in memory
# notice: no more missing values in X
any(is.na(pen$genotypes[,]))
```

To prepare for data analysis, we need to use the data to create the pieces we need for our model: a design matrix $\mathbf{X}$, an outcome vector $\mathbf{y}$, and a the vector with the penalty factor indicators (1 = feature will be penalized, 0 = feature will not be penalized). 

```{r}
# get outcome data 
penncath_pheno <- read.csv(find_example_data(path = 'penncath_clinical.csv'))

phen <- data.frame(FamID = penncath_pheno$FamID, CAD = penncath_pheno$CAD) |>
  mutate(FamID = as.character(FamID))

pen_design <- create_design(dat_file = plink_data,
                            feature_id = "FID",
                            rds_dir = temp_dir,
                            new_file = "std_penncath_lite",
                            add_outcome = phen,
                            outcome_id = "FamID",
                            outcome_col = "CAD",
                            overwrite = TRUE,
                            logfile = "design")
pen_design_rds <- readRDS(pen_design)

# we can check to see that our data have been standardized 
std_X <- attach.big.matrix(pen_design_rds$std_X)
colMeans(std_X[,]) |> summary() # columns have mean zero...
apply(std_X[,], 2, var) |> summary() # ... & variance 1
```

## Step 2: fit a model

Note: for data coming from `process_plink()`, the `y` argument in `plmm()` defaults to using the 6th column from the '.fam' file as the phenotype/outcome (this behavior is the same as what PLINK does).

```{r}
fb_fit <- plmm(design = pen_design,
               returnX = FALSE,
               # this datset is small enough to fit in memory
               # by setting returnX = FALSE, I force plmm() to run on the filebacked data
               save_rds = file.path(temp_dir, "fb_fit"),
               # save results to my temp directory, and name files 'fb_fit'
               trace = TRUE)
```


We can confirm that our filebacked methods and in-memory methods give the same results:
```{r}
fit <- plmm(design = pen_design, # will run in-memory by default, since data will fit
            trace = TRUE)

b1 <- fb_fit$beta_vals |> as.matrix()
b2 <- fit$beta_vals
tinytest::expect_equivalent(b1, b2, 0.01) 
```

## Step 3: Plot/summarize results

To summarize a single model (no CV), we can look at a plot of the coefficient paths:

```{r}
plot(fb_fit)
```

We can also examine the number of selected variables at a particular index of $\lambda$:

```{r}
summary(fb_fit, idx = 50)
```


## Step 4: Cross-validation (tuning parameter selection)

In most applications, we will want to fit a model using cross validation in order to choose an appropriate value of $\lambda$, the tuning parameter for our penalized model.

```{r}
cv_fb_fit <- cv_plmm(design = pen_design,
                     type = 'blup',
                     returnX = FALSE,
                     nfolds = 3, # for sake of testing, I made this small. 
                     # In practice, 3 folds is usually not enough for robust 
                     # cross validation - hence, the default is nfolds = 5
                     returnBiasDetails = TRUE,
                     trace = TRUE,
                     save_rds = file.path(bigmemory::dir.name(bigmemory::attach.big.matrix(pen_design_rds$std_X)),
                                          "cv"), # save_results to save folder as data 
                     save_fold_res = TRUE, # saves loss and predictions in each fold
                     compact_save = TRUE, # saves results in separate files
                     seed = 123)

```

Fitting an equivalent model using the `penncath_lite` data that uses RAM could look like this: 

```{r, eval=FALSE}
cv_fit <- cv_plmm(design = pen_design_rds,
                  type = 'blup',
                  nfolds = 3,
                  returnBiasDetails = TRUE,
                  trace = TRUE,
                  seed = 123)

# the tests below confirm that results are the same for the filebacked and in-memory CV fits
tinytest::expect_equivalent(cv_fb_fit$cve, cv_fit$cve, tolerance = 0.01)
tinytest::expect_equivalent(cv_fb_fit$Loss[,cv_fb_fit$min],
                            cv_fit$Loss[,cv_fit$min],
                            tolerance = 0.01)
```

To summarize and plot a CV fit, we can look at a plot of cross validation error:

```{r}
plot(cv_fb_fit)
```

To summarize and visualize the model at the chosen $\lambda$ parameter value, we can do this:

```{r}
summary(cv_fb_fit)
```

## Comparing candidate models

The default penalty in `plmmr` is the lasso. Another option would be the minimax-concave penalty (MCP)[^2]. Suppose we want to compare our lasso-penalized model results with a MCP-penalized model. We can fit a lasso model as shown:

[^1]: Zhang, Cun-Hui. "Nearly unbiased variable selection under minimax concave penalty." (2010): 894-942.

```{r}
fb_mcp <- plmm(design = pen_design,
                 penalty = "MCP",
                 returnX = FALSE,
                 trace = TRUE)

plot(fb_mcp)
summary(fb_mcp, idx = 50)
```

## To add predictors that are not genomic

In many biological applications, we want to include some features/covariates/predictors in addition to the genomic information, like clinical or demographic information. Let's suppose that in the `penncath` data analysis, we want to include 'sex' and 'age' as unpenalized covariates. We can do this by supplying an additional argument to `create_design()`:

```{r}
extdata <- penncath_pheno |> 
  dplyr::select(FamID, sex, age) |>
  dplyr::mutate(FamID = as.character(FamID))
any(is.na(extdata)) # important: additional predictors cannot have any missing values

# create a new design, using the additional predictors this time
new_pen_design <- create_design(dat_file = plink_data,
                            feature_id = "FID",
                            rds_dir = temp_dir,
                            new_file = "std_penn_lit_plus_pred",
                            add_outcome = phen,
                            outcome_id = "FamID",
                            outcome_col = "CAD",
                            # the lines below are what's new here: 
                            add_predictor = extdata,
                            predictor_id = 'FamID',
                            overwrite = TRUE,
                            logfile = "design_with_pred")
# check this out: 
pen2 <- readRDS(new_pen_design)
pen2$std_X_colnames |> head() # std_X includes our non-genomic covariates 
```

We usually want these kinds of variables to always be in the model (i.e., they are not penalized). To make sure 'sex' and 'age' are always included in the chosen model, let's set `penalty_factor` to have 0 values corresponding to these predictors.

```{r}
fit_plus_newvars <- plmm(design = new_pen_design,
                         K = fb_fit$K, 
                         returnX = FALSE,
                         trace = TRUE)
```

We can check out how these additional covariates impact model fit:

```{r}
plot(fit_plus_newvars)

summary(fb_fit, idx = 1) # at highest penalty value, nothing in the original model
summary(fit_plus_newvars, idx = 1) # at highest penalty value, sex and age are still in the model

# look at the estimated coefficients 
fb_fit$beta_vals[1:10, 1:5]
fit_plus_newvars$beta_vals[1:10, 1:5]
```
